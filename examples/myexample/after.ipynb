{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "widespread-klein",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensortrade.oms.instruments import Instrument, BTC, USD\n",
    "from tensortrade.env.default.actions import BSH\n",
    "\n",
    "from tensortrade.env.generic import Renderer\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "import ray\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import tensortrade.env.default as default\n",
    "\n",
    "from tensortrade.feed.core import DataFeed, Stream\n",
    "from tensortrade.oms.exchanges import Exchange\n",
    "from tensortrade.oms.exchanges import ExchangeOptions\n",
    "from tensortrade.oms.services.execution.simulated import execute_order\n",
    "from tensortrade.oms.wallets import Wallet, Portfolio\n",
    "\n",
    "from tensortrade.data.cdd import CryptoDataDownload\n",
    "\n",
    "from ray import tune\n",
    "from ray.tune.registry import register_env\n",
    "import ray.rllib.agents.ppo as ppo\n",
    "\n",
    "import ray\n",
    "from ray.rllib.utils.filter import MeanStdFilter\n",
    "\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "packed-quick",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_env(config, train=\"train\"):\n",
    "    cdd = CryptoDataDownload()\n",
    "    data = cdd.fetch(\"Bitstamp\", \"USD\", \"BTC\", \"1h\")\n",
    "    if False:\n",
    "        data.close = data.close / 20 + range(len(data))\n",
    "        print(\"genenrating fake increase\")\n",
    "    if train == \"train\":\n",
    "        data = data[0:int(len(data) / 2)]  # training\n",
    "        print(\"using first half for training\")\n",
    "    elif train == \"eval\":\n",
    "        data = data[int(len(data) / 2):]  # validation\n",
    "        print(\"using second half for eval\")\n",
    "    else:\n",
    "        print(\"using all data\")\n",
    "\n",
    "    pclose = Stream.source(list(data.close), dtype=\"float\").rename(\"USD-BTC\")\n",
    "    pmin = Stream.source(list(data.low), dtype=\"float\").rename(\"USD-BTClow\")\n",
    "    pmax = Stream.source(list(data.high), dtype=\"float\").rename(\"USD-BTChigh\")\n",
    "\n",
    "    pmin = Stream.source(list(data.low), dtype=\"float\").rename(\"USD-BTClow\")\n",
    "    pmax = Stream.source(list(data.high), dtype=\"float\").rename(\"USD-BTChigh\")\n",
    "\n",
    "    pmin3 = pmin.rolling(window=3).min()\n",
    "    pmin10 = pmin.rolling(window=10).min()\n",
    "    pmin20 = pmin.rolling(window=20).min()\n",
    "    pmax3 = pmax.rolling(window=3).max()\n",
    "    pmax10 = pmax.rolling(window=10).max()\n",
    "    pmax20 = pmax.rolling(window=20).max()\n",
    "\n",
    "    eo = ExchangeOptions(commission=0.002)  #\n",
    "    coinbase = Exchange(\"coinbase\", service=execute_order, options=eo)(\n",
    "        pclose\n",
    "    )\n",
    "\n",
    "    cash = Wallet(coinbase, 100000 * USD)\n",
    "    asset = Wallet(coinbase, 0 * BTC)\n",
    "\n",
    "    portfolio = Portfolio(USD, [\n",
    "        cash,\n",
    "        asset\n",
    "    ])\n",
    "\n",
    "    feed = DataFeed([\n",
    "\n",
    "        (pclose.log() - pmin3.log()).fillna(0).rename(\"relmin3\"),\n",
    "        (pclose.log() - pmin10.log()).fillna(0).rename(\"relmin10\"),\n",
    "        (pclose.log() - pmin20.log()).fillna(0).rename(\"relmin20\"),\n",
    "        (pclose.log() - pmax3.log()).fillna(0).rename(\"relmax3\"),\n",
    "        (pclose.log() - pmax10.log()).fillna(0).rename(\"relmax10\"),\n",
    "        (pclose.log() - pmax20.log()).fillna(0).rename(\"relmax20\"),\n",
    "\n",
    "    ])\n",
    "\n",
    "    action_scheme = BSH(cash=cash, asset=asset)\n",
    "\n",
    "    renderer_feed = DataFeed([\n",
    "        Stream.source(list(data.close), dtype=\"float\").rename(\"price\"),\n",
    "        Stream.sensor(action_scheme, lambda s: s.action, dtype=\"float\").rename(\"action\")  # only works for BSH\n",
    "    ])\n",
    "\n",
    "    environment = default.create(\n",
    "\n",
    "        feed=feed,\n",
    "        portfolio=portfolio,\n",
    "        action_scheme=action_scheme,\n",
    "        reward_scheme=\"simple\",\n",
    "        renderer_feed=renderer_feed,\n",
    "        renderer=PositionChangeChart(),\n",
    "        window_size=config[\"window_size\"],\n",
    "        min_periods=20,\n",
    "        max_allowed_loss=0.6\n",
    "    )\n",
    "    return environment\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bridal-timothy",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-02-14 22:19:48,774\tINFO services.py:1172 -- View the Ray dashboard at \u001b[1m\u001b[32mhttp://127.0.0.1:8265\u001b[39m\u001b[22m\n"
     ]
    }
   ],
   "source": [
    "# Get checkpoint\n",
    "\n",
    "window_size = 30\n",
    "pname = \"PPO_TradingEnv_0e8da_00000_0_2021-02-14_18-35-40\"\n",
    "# c:\\work\\klemen\\rlagent\\Experiments\\PPO\\PPO_TradingEnv_0e8da_00000_0_2021-02-14_18-35-40\n",
    "checkpoint_path = \"c:/work/klemen/rlagent/Experiments/PPO/\" + pname + \"/checkpoint_210/checkpoint-210\"\n",
    "\n",
    "if ray.is_initialized() == False:\n",
    "    ray.init()\n",
    "else:\n",
    "    ray.init(address=\"auto\", include_webui=False)  # for local debugging\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dense-fetish",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Restore agent\n",
    "agent = ppo.PPOTrainer(\n",
    "    env=\"TradingEnv\",\n",
    "    config={\n",
    "        \"env_config\": {\n",
    "            \"window_size\": window_size\n",
    "        },\n",
    "        \"model\": {\n",
    "            # Share layers for value function. If you set this to True, it's\n",
    "            # important to tune vf_loss_coeff.\n",
    "            # \"vf_share_layers\": True,\n",
    "            \"vf_share_layers\": False,\n",
    "            \"fcnet_hiddens\": [32, 16, 16],\n",
    "\n",
    "            \"use_lstm\": True,\n",
    "            # Max seq len for training the LSTM, defaults to 20.\n",
    "            \"max_seq_len\": 20,\n",
    "            # Size of the LSTM cell.\n",
    "            # \"lstm_cell_size\": 256,\n",
    "            \"lstm_cell_size\": 32,\n",
    "            # Whether to feed a_{t-1} to LSTM (one-hot encoded if discrete).\n",
    "            \"lstm_use_prev_action\": False,  # TODO: play with this\n",
    "            # Whether to feed r_{t-1} to LSTM.\n",
    "            \"lstm_use_prev_reward\": False,\n",
    "            # Experimental (only works with `_use_trajectory_view_api`=True):\n",
    "            # Whether the LSTM is time-major (TxBx..) or batch-major (BxTx..).\n",
    "            \"_time_major\": False,\n",
    "        },\n",
    "        \"log_level\": \"DEBUG\",\n",
    "        \"framework\": \"torch\",\n",
    "        \"ignore_worker_failures\": True,\n",
    "        # \"num_workers\": 3,  #max\n",
    "        \"num_workers\": 3,\n",
    "        'num_gpus': 1,\n",
    "        \"clip_rewards\": False,\n",
    "\n",
    "        \"gamma\": 0,\n",
    "\n",
    "        \"observation_filter\": \"MeanStdFilter\",\n",
    "        # \"observation_filter\": MyStdFilter,  # demean=False, destd=True rewrd = -3.04045e+06 after 1000\n",
    "        # \"observation_filter\": \"NoFilter\",     # => reward -1.12313e+08 after 1000 iterations\n",
    "\n",
    "        \"lambda\": 0.72,\n",
    "        # \"vf_loss_coeff\": 0.5,\n",
    "        \"vf_loss_coeff\": 1.0,\n",
    "        # \"entropy_coeff\": 0.01\n",
    "        \"entropy_coeff\": 0.1,\n",
    "\n",
    "        # 'rollout_fragment_length': 300,\n",
    "    }\n",
    ")\n",
    "agent.restore(checkpoint_path)\n",
    "\n",
    "# Visualization\n",
    "# Instantiate the environment\n",
    "env = create_env({\n",
    "    \"window_size\": window_size\n",
    "}, \"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "polar-liberty",
   "metadata": {},
   "outputs": [],
   "source": [
    "episode_reward = 0\n",
    "done = False\n",
    "obs = env.reset()\n",
    "\n",
    "j = 0\n",
    "\n",
    "state = agent.get_policy().model.get_initial_state()\n",
    "\n",
    "while not done:\n",
    "    action, state, logits = agent.compute_action(obs, state=state)\n",
    "    obs, reward, done, info = env.step(action)\n",
    "    print(j, action, reward, done, info)\n",
    "    episode_reward += reward\n",
    "    j += 1\n",
    "\n",
    "env.render()\n",
    "\n",
    "###\n",
    "# Instantiate the environment\n",
    "env = create_env({\n",
    "    \"window_size\": window_size\n",
    "}, \"eval\")\n",
    "\n",
    "episode_reward = 0\n",
    "done = False\n",
    "obs = env.reset()\n",
    "\n",
    "j = 0\n",
    "\n",
    "state = agent.get_policy().model.get_initial_state()\n",
    "\n",
    "while not done:\n",
    "    action, state, logits = agent.compute_action(obs, state=state)\n",
    "    obs, reward, done, info = env.step(action)\n",
    "    print(j, action, reward, done, info)\n",
    "    episode_reward += reward\n",
    "    j += 1\n",
    "\n",
    "env.render()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
